{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-community langsmith langchain-experimental sentence_transformers tavily-python pandas fsspec huggingface_hub langchain-mistralai nltk langchain-ollama\n",
    "%pip install 'urllib3<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"test_langgraph\"\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from langchain_core.tools import tool\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"hf://datasets/yixuantt/MultiHopRAG/MultiHopRAG.json\")\n",
    "corpus = pd.read_json(\"hf://datasets/yixuantt/MultiHopRAG/corpus.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the figure associated with generative AI technology whose departure from OpenAI was considered shocking according to Fortune, and is also the subject of a prevailing theory suggesting a lack of full truthfulness with the board as reported by TechCrunch?\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "print(df['query'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>published_at</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200+ of the best deals from Amazon's Cyber Mon...</td>\n",
       "      <td>None</td>\n",
       "      <td>Mashable</td>\n",
       "      <td>2023-11-27 08:45:59+00:00</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>https://mashable.com/article/cyber-monday-deal...</td>\n",
       "      <td>Table of Contents Table of Contents Echo, Fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASX set to drop as Wall Street’s September slu...</td>\n",
       "      <td>Stan Choe</td>\n",
       "      <td>The Sydney Morning Herald</td>\n",
       "      <td>2023-09-26 19:11:30+00:00</td>\n",
       "      <td>business</td>\n",
       "      <td>https://www.smh.com.au/business/markets/asx-se...</td>\n",
       "      <td>ETF provider Betashares, which manages $30 bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon sellers sound off on the FTC's 'long-ov...</td>\n",
       "      <td>None</td>\n",
       "      <td>Cnbc | World Business News Leader</td>\n",
       "      <td>2023-10-06 21:31:00+00:00</td>\n",
       "      <td>business</td>\n",
       "      <td>https://www.cnbc.com/2023/10/06/amazon-sellers...</td>\n",
       "      <td>A worker sorts out parcels in the outbound doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christmas Day preview: 49ers, Ravens square of...</td>\n",
       "      <td>Colum Dell, Yardbarker</td>\n",
       "      <td>Yardbarker</td>\n",
       "      <td>2023-12-24 23:34:39+00:00</td>\n",
       "      <td>sports</td>\n",
       "      <td>https://www.yardbarker.com/nfl/articles/christ...</td>\n",
       "      <td>Christmas Day isn't just for the NBA, as the N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Raiders vs. Lions live score, updates, highlig...</td>\n",
       "      <td>Dan Treacy</td>\n",
       "      <td>Sporting News</td>\n",
       "      <td>2023-10-30 22:20:03+00:00</td>\n",
       "      <td>sports</td>\n",
       "      <td>https://www.sportingnews.com/us/nfl/news/raide...</td>\n",
       "      <td>The Lions just needed to get themselves back i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  author  \\\n",
       "0  200+ of the best deals from Amazon's Cyber Mon...                    None   \n",
       "1  ASX set to drop as Wall Street’s September slu...               Stan Choe   \n",
       "2  Amazon sellers sound off on the FTC's 'long-ov...                    None   \n",
       "3  Christmas Day preview: 49ers, Ravens square of...  Colum Dell, Yardbarker   \n",
       "4  Raiders vs. Lions live score, updates, highlig...              Dan Treacy   \n",
       "\n",
       "                              source              published_at       category  \\\n",
       "0                           Mashable 2023-11-27 08:45:59+00:00  entertainment   \n",
       "1          The Sydney Morning Herald 2023-09-26 19:11:30+00:00       business   \n",
       "2  Cnbc | World Business News Leader 2023-10-06 21:31:00+00:00       business   \n",
       "3                         Yardbarker 2023-12-24 23:34:39+00:00         sports   \n",
       "4                      Sporting News 2023-10-30 22:20:03+00:00         sports   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://mashable.com/article/cyber-monday-deal...   \n",
       "1  https://www.smh.com.au/business/markets/asx-se...   \n",
       "2  https://www.cnbc.com/2023/10/06/amazon-sellers...   \n",
       "3  https://www.yardbarker.com/nfl/articles/christ...   \n",
       "4  https://www.sportingnews.com/us/nfl/news/raide...   \n",
       "\n",
       "                                                body  \n",
       "0  Table of Contents Table of Contents Echo, Fire...  \n",
       "1  ETF provider Betashares, which manages $30 bil...  \n",
       "2  A worker sorts out parcels in the outbound doc...  \n",
       "3  Christmas Day isn't just for the NBA, as the N...  \n",
       "4  The Lions just needed to get themselves back i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/terra/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('punkt_tab')\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "loader = DataFrameLoader(corpus, page_content_column=\"body\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Helper function to split document body into sentences\n",
    "def split_into_sentences(doc):\n",
    "    return sent_tokenize(doc.page_content)\n",
    "\n",
    "# VectorStoreRetriever class to store and query sentences\n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self, docs: list, vectors: list, sentences: list):\n",
    "        self._arr = np.array(vectors)  # Embedding vectors for sentences\n",
    "        self._docs = docs  # Original documents\n",
    "        self._sentences = sentences  # List of sentence-level content\n",
    "\n",
    "    @classmethod\n",
    "    def from_docs(cls, docs):\n",
    "        all_sentences = []\n",
    "        \n",
    "        # Process each document: split it into sentences, generate embeddings\n",
    "        for doc in docs:\n",
    "            sentences = split_into_sentences(doc)  # Split into sentences\n",
    "            all_sentences.extend(sentences)\n",
    "        \n",
    "        # Generate embeddings for the sentences\n",
    "        sentence_embeddings = hf.embed_documents(all_sentences)\n",
    "        return cls(docs, sentence_embeddings, all_sentences)\n",
    "\n",
    "    def query(self, query: str, k: int = 5) -> list[dict]:\n",
    "        # Embed the query\n",
    "        embed = hf.embed_query(query)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        scores = np.array(embed) @ self._arr.T\n",
    "        top_k_idx = np.argpartition(scores, -k)[-k:]\n",
    "        top_k_idx_sorted = top_k_idx[np.argsort(-scores[top_k_idx])]\n",
    "        \n",
    "        # Return the top K similar sentences\n",
    "        return [\n",
    "            {\"sentence\": self._sentences[idx], \"similarity\": scores[idx]}\n",
    "            for idx in top_k_idx_sorted\n",
    "        ]\n",
    "\n",
    "# Initialize the retriever with sentence-level embeddings\n",
    "#retriever = VectorStoreRetriever.from_docs(documents)\n",
    "\n",
    "retriever = None \n",
    "\n",
    "def initialize_retriever():\n",
    "    model_name = \"BAAI/bge-small-en\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    hf = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    # Load corpus\n",
    "    corpus = pd.read_json(\"hf://datasets/yixuantt/MultiHopRAG/corpus.json\")\n",
    "    loader = DataFrameLoader(corpus, page_content_column=\"body\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Process documents into sentences\n",
    "    all_sentences = []\n",
    "    for doc in docs:\n",
    "        sentences = split_into_sentences(doc)\n",
    "        all_sentences.extend(sentences)\n",
    "\n",
    "    # Generate embeddings\n",
    "    sentence_embeddings = hf.embed_documents(all_sentences)\n",
    "\n",
    "    # Create the retriever\n",
    "    retriever = VectorStoreRetriever(docs, sentence_embeddings, all_sentences)\n",
    "\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = initialize_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def find_answer_in_docs(query: str) -> str:\n",
    "    \"\"\"do RAG\"\"\"\n",
    "    # Perform query and get the most relevant sentences\n",
    "    docs = retriever.query(query, k=4)\n",
    "    \n",
    "    # Return the retrieved sentences as a string\n",
    "    retrieval_results = \"\\n\\n\".join([doc[\"sentence\"] for doc in docs])\n",
    "    return retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "@tool \n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"search the web\"\"\"\n",
    "    tavily_client = TavilyClient(api_key=\"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\")\n",
    "    response = tavily_client.search(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "         #   print(\"TYPE: \", message.type) #human, ai, tool\n",
    "         #   print(\"CONTENT: \", message.content)\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)\n",
    "            \n",
    "\n",
    "def add_to_results(event: dict, results: dict):\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "         if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "         if message.content != \"\":\n",
    "            results[message.type] = message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state \n",
    "Define our StateGraph's state as a typed dictionary containing an append-only list of messages. These messages form the chat history, which is all the state our simple assistant needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent¶\n",
    "\n",
    "Next, define the assistant function. This function takes the graph state, formats it into a prompt, and then calls an LLM for it to predict the best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g7/xc8nt9ms647dgxyl0kpl62b00000gq/T/ipykernel_11502/3933804469.py:33: LangChainBetaWarning: Introduced in 0.2.24. API subject to change.\n",
      "  my_rate_limiter = InMemoryRateLimiter(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from datetime import date, datetime\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            configuration = config.get(\"configurable\", {})\n",
    "        #    passenger_id = configuration.get(\"passenger_id\", None)\n",
    "            state = {**state}\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "part_1_tools = [find_answer_in_docs, search_web]\n",
    "\n",
    "my_rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second = .05,\n",
    "    check_every_n_seconds = 0.5,\n",
    "    max_bucket_size = 5\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    "    max_retries=3,\n",
    "    rate_limiter=my_rate_limiter,\n",
    "    timeout = 240\n",
    "    # other params...\n",
    ").bind_tools(part_1_tools)\n",
    "\n",
    "\n",
    "llm2 = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=3,\n",
    "    rate_limiter=my_rate_limiter,\n",
    "    timeout = 240\n",
    "    # other params...\n",
    ").bind_tools(part_1_tools)\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a question answering assistant. \"\n",
    "            \"Invoke the tools given to you when needed to respond to user queries.\"\n",
    "            \"Use the documents given to you for any questions on English magazines such as 'The Age', 'Fortune', 'The Verge' and 'TechCrunch'\"\n",
    "            \" Provide a really short answer.\"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"All queries have an answer.\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now)\n",
    "\n",
    "\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm2 #.bind_tools(part_1_tools)\n",
    "\n",
    "def build_part_1_assistant_runnable():\n",
    "\n",
    "    part_1_tools = [find_answer_in_docs, search_web]\n",
    "\n",
    "    my_rate_limiter = InMemoryRateLimiter(\n",
    "        requests_per_second = .05,\n",
    "        check_every_n_seconds = 0.5,\n",
    "        max_bucket_size = 5\n",
    "    )\n",
    "    \n",
    "    llm2 = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=3,\n",
    "    rate_limiter=my_rate_limiter,\n",
    "    timeout = 240\n",
    "    # other params...\n",
    "    ).bind_tools(part_1_tools)\n",
    "\n",
    "    primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a question answering assistant. \"\n",
    "                \"Invoke the tools given to you when needed to respond to user queries.\"\n",
    "                \"Use the documents given to you for any questions on English magazines such as 'The Age', 'Fortune', 'The Verge' and 'TechCrunch'\"\n",
    "                \" Provide a really short answer.\"\n",
    "                \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "                \" If a search comes up empty, expand your search before giving up.\"\n",
    "                \"All queries have an answer.\"\n",
    "                \"\\nCurrent time: {time}.\",\n",
    "            ),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    ).partial(time=datetime.now)\n",
    "\n",
    "    return primary_assistant_prompt | llm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Graph¶\n",
    "\n",
    "Now, create the graph. The graph is the final assistant for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = MemorySaver()\n",
    "part_1_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "def get_part_1_graph():\n",
    "    return part_1_graph\n",
    "\n",
    "def initialize_part_1_graph():\n",
    "\n",
    "    builder = StateGraph(State)\n",
    "    part_1_tools =  [find_answer_in_docs, search_web]\n",
    "    memory = MemorySaver()\n",
    "\n",
    "    builder.add_node(\"assistant\", Assistant(build_part_1_assistant_runnable()))\n",
    "    builder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n",
    "    builder.add_edge(START, \"assistant\")\n",
    "    builder.add_conditional_edges(\n",
    "        \"assistant\",\n",
    "        tools_condition,\n",
    "    )\n",
    "    builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "    memory = MemorySaver()\n",
    "    part_1_graph = builder.compile(checkpointer=memory)\n",
    "    return part_1_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_1_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example conversation\n",
    "\n",
    "get questionds from MultiHopRAG df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "#tutorial_questions = df[\"query\"][:5] \n",
    "tutorial_questions = [\"hello, can you help wish some questions?\",\"find an open restaurant in edinburgh\", df[\"query\"][0]]\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "_printed = set()\n",
    "#for question in tutorial_questions:\n",
    "while True:\n",
    "    question = input(\"Your message: \")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    result = {}\n",
    "    events = part_1_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "        add_to_results(event, result)\n",
    "    results.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ai': 'Hi! How can I assist you today?', 'human': 'hi'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "pp.pprint(results)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithConflictError",
     "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/utils.py:150\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/demo/.venv/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/client.py:763\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    754\u001b[0m         method,\n\u001b[1;32m    755\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_kwargs,\n\u001b[1;32m    762\u001b[0m     )\n\u001b[0;32m--> 763\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/utils.py:152\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLangSmithConflictError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define dataset: these are your test cases\u001b[39;00m\n\u001b[1;32m     11\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy Sample Dataset 3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA sample dataset in LangSmith.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m client\u001b[38;5;241m.\u001b[39mcreate_examples(\n\u001b[1;32m     14\u001b[0m   inputs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     15\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is the figure associated with generative AI technology whose departure from OpenAI was considered shocking according to Fortune, and is also the subject of a prevailing theory suggesting a lack of full truthfulness with the board as reported by TechCrunch?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m   dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/client.py:2817\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[0;34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, metadata)\u001b[0m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2815\u001b[0m     dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs_schema_definition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs_schema\n\u001b[0;32m-> 2817\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/datasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_orjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2823\u001b[0m ls_utils\u001b[38;5;241m.\u001b[39mraise_for_status_with_text(response)\n\u001b[1;32m   2825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   2826\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[1;32m   2827\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   2828\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[1;32m   2829\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/client.py:808\u001b[0m, in \u001b[0;36mClient.request_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    806\u001b[0m     )\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithConflictError(\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     )\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithError(\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in LangSmith\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m API. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     )\n",
      "\u001b[0;31mLangSmithConflictError\u001b[0m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
     ]
    }
   ],
   "source": [
    "from langsmith import Client, evaluate\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.schemas import Example, Run\n",
    "from langsmith.evaluation import EvaluationResult\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"My Sample Dataset 3\"\n",
    "dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")\n",
    "client.create_examples(\n",
    "  inputs=[\n",
    "      {\"query\": \"Who is the figure associated with generative AI technology whose departure from OpenAI was considered shocking according to Fortune, and is also the subject of a prevailing theory suggesting a lack of full truthfulness with the board as reported by TechCrunch?\"},\n",
    "      {\"query\": \"recommend a cinema in edinburgh open today\"},\n",
    "      {\"query\": \"thank you for helping me!\"}\n",
    "  ],\n",
    "  outputs=[\n",
    "      {\"output\": \"Sam Altman\"},\n",
    "      {\"output\": \"Vue\"},\n",
    "      {\"output\": \"You're welcome\"}\n",
    "  ],\n",
    "  dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sample-experiment-e5135955' at:\n",
      "https://smith.langchain.com/o/748f64cf-3528-45fb-82ff-ef191d9b20b3/datasets/8f04f5f5-061b-4b2f-a999-e5d69f705f07/compare?selectedSessions=885753ea-5c14-451a-b145-99477f151a51\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running target function: Expected dict, got [('user', 'recommend a cinema in edinburgh open today')]\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1675, in _forward\n",
      "    fn(example.inputs, langsmith_extra=langsmith_extra)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/g7/xc8nt9ms647dgxyl0kpl62b00000gq/T/ipykernel_9620/944510006.py\", line 4, in predict_agent_messages\n",
      "    messages = part_1_graph.invoke(msg, config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1927, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 104, in tick\n",
      "    run_with_retry(t, retry_policy, writer=writer)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n",
      "    task.proc.invoke(task.input, config)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 176, in invoke\n",
      "    ret = context.run(self.func, input, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 85, in _write\n",
      "    self.do_write(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 129, in do_write\n",
      "    values = [\n",
      "             ^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 130, in <listcomp>\n",
      "    write.mapper(write.value) if write.mapper is not None else write.value\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/graph/state.py\", line 649, in _get_state_key\n",
      "    raise InvalidUpdateError(msg)\n",
      "langgraph.errors.InvalidUpdateError: Expected dict, got [('user', 'recommend a cinema in edinburgh open today')]\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "Error running target function: Expected dict, got [('user', 'thank you for helping me!')]\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1675, in _forward\n",
      "    fn(example.inputs, langsmith_extra=langsmith_extra)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/g7/xc8nt9ms647dgxyl0kpl62b00000gq/T/ipykernel_9620/944510006.py\", line 4, in predict_agent_messages\n",
      "    messages = part_1_graph.invoke(msg, config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1927, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 104, in tick\n",
      "    run_with_retry(t, retry_policy, writer=writer)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n",
      "    task.proc.invoke(task.input, config)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 176, in invoke\n",
      "    ret = context.run(self.func, input, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 85, in _write\n",
      "    self.do_write(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 129, in do_write\n",
      "    values = [\n",
      "             ^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 130, in <listcomp>\n",
      "    write.mapper(write.value) if write.mapper is not None else write.value\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/graph/state.py\", line 649, in _get_state_key\n",
      "    raise InvalidUpdateError(msg)\n",
      "langgraph.errors.InvalidUpdateError: Expected dict, got [('user', 'thank you for helping me!')]\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "Error running target function: Expected dict, got [('user', 'Who is the figure associated with generative AI technology whose departure from OpenAI was considered shocking according to Fortune, and is also the subject of a prevailing theory suggesting a lack of full truthfulness with the board as reported by TechCrunch?')]\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1675, in _forward\n",
      "    fn(example.inputs, langsmith_extra=langsmith_extra)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/g7/xc8nt9ms647dgxyl0kpl62b00000gq/T/ipykernel_9620/944510006.py\", line 4, in predict_agent_messages\n",
      "    messages = part_1_graph.invoke(msg, config)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1927, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 104, in tick\n",
      "    run_with_retry(t, retry_policy, writer=writer)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n",
      "    task.proc.invoke(task.input, config)\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 176, in invoke\n",
      "    ret = context.run(self.func, input, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 85, in _write\n",
      "    self.do_write(\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 129, in do_write\n",
      "    values = [\n",
      "             ^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/pregel/write.py\", line 130, in <listcomp>\n",
      "    write.mapper(write.value) if write.mapper is not None else write.value\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langgraph/graph/state.py\", line 649, in _get_state_key\n",
      "    raise InvalidUpdateError(msg)\n",
      "langgraph.errors.InvalidUpdateError: Expected dict, got [('user', 'Who is the figure associated with generative AI technology whose departure from OpenAI was considered shocking according to Fortune, and is also the subject of a prevailing theory suggesting a lack of full truthfulness with the board as reported by TechCrunch?')]\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n",
      "Error running evaluator <DynamicRunEvaluator contains_answer> on run c9f57cf1-be70-48c5-a674-3607b3d7f226: TypeError(\"argument of type 'NoneType' is not iterable\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1404, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/g7/xc8nt9ms647dgxyl0kpl62b00000gq/T/ipykernel_9620/944510006.py\", line 11, in contains_answer\n",
      "    if example.outputs[\"output\"] in run.outputs[\"output\"]:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: argument of type 'NoneType' is not iterable\n",
      "Error running evaluator <DynamicRunEvaluator contains_answer> on run b00fea68-7b2b-46ab-9556-92f602ab5a73: TypeError(\"argument of type 'NoneType' is not iterable\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1404, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/g7/xc8nt9ms647dgxyl0kpl62b00000gq/T/ipykernel_9620/944510006.py\", line 11, in contains_answer\n",
      "    if example.outputs[\"output\"] in run.outputs[\"output\"]:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: argument of type 'NoneType' is not iterable\n",
      "Error running evaluator <DynamicRunEvaluator contains_answer> on run b3114c80-eba3-4740-a6bf-be6c2c8159b0: TypeError(\"argument of type 'NoneType' is not iterable\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1404, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/terra/Desktop/demo/.venv/lib/python3.11/site-packages/langsmith/run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/g7/xc8nt9ms647dgxyl0kpl62b00000gq/T/ipykernel_9620/944510006.py\", line 11, in contains_answer\n",
      "    if example.outputs[\"output\"] in run.outputs[\"output\"]:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vue\n",
      "{'output': None}\n",
      "You're welcome\n",
      "{'output': None}\n",
      "Sam Altman\n",
      "{'output': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  5.46it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_agent_messages(example: dict):\n",
    "   \"\"\"Invoke assistant for single tool call evaluation\"\"\"\n",
    "   msg = [ (\"user\", example[\"query\"]) ]\n",
    "   messages = part_1_graph.invoke(msg, config)\n",
    "   return {\"response\": messages['messages'][-1].content}\n",
    "\n",
    "# Define your evaluator\n",
    "def contains_answer(run, example):\n",
    "   print(example.outputs[\"output\"])\n",
    "   print(run.outputs)\n",
    "   if example.outputs[\"output\"] in run.outputs[\"output\"]:\n",
    "      score = 1\n",
    "   else:\n",
    "      score = 0\n",
    "   return EvaluationResult(\n",
    "        key=\"contains answer\",\n",
    "        score=score,\n",
    "    )\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "  predict_agent_messages, # Your AI system goes here\n",
    "  data=dataset_name, # The data to predict and grade over\n",
    "  evaluators=[contains_answer], # The evaluators to score the results\n",
    "  experiment_prefix=\"sample-experiment\", # The name of the experiment\n",
    "  metadata={\n",
    "    \"version\": \"1.0.1\",\n",
    "    \"revision_id\": \"beta\"\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "globals()[\"part_1_graph\"] = part_1_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in ./.venv/lib/python3.11/site-packages (1.40.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in ./.venv/lib/python3.11/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in ./.venv/lib/python3.11/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in ./.venv/lib/python3.11/site-packages (from streamlit) (5.29.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./.venv/lib/python3.11/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./.venv/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in ./.venv/lib/python3.11/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./.venv/lib/python3.11/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in ./.venv/lib/python3.11/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./.venv/lib/python3.11/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in ./.venv/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (1.15.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 22:44:49.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-02 22:44:49.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import uuid\n",
    "\n",
    "\n",
    "# Initialize session state for the conversation\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "if \"results\" not in st.session_state:\n",
    "    st.session_state[\"results\"] = []\n",
    "\n",
    "# Generate unique thread ID for each session\n",
    "if \"thread_id\" not in st.session_state:\n",
    "    st.session_state[\"thread_id\"] = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "# Configuration for LangGraph\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": st.session_state[\"thread_id\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "st.title(\"LangGraph Chatbot Demo\")\n",
    "st.info(\"Type your message below and interact with the LangGraph agent. Type 'exit' or 'quit' to end the session.\")\n",
    "\n",
    "# Chat interface\n",
    "user_input = st.text_input(\"Your message:\", key=\"user_input\")\n",
    "\n",
    "if user_input:\n",
    "    # Exit condition\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        st.write(\"Session ended. Refresh the page to start again.\")\n",
    "        st.stop()\n",
    "\n",
    "    # Process user input\n",
    "    st.session_state[\"messages\"].append((\"User\", user_input))\n",
    "    result = {}\n",
    "    graph = initialize_part_1_graph()\n",
    "    events = graph.stream(\n",
    "        {\"messages\": (\"user\", user_input)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "\n",
    "    # Handle streaming events\n",
    "    _printed = set()\n",
    "    assistant_response = \"\"\n",
    "    tool_output = \"\"\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "        add_to_results(event, result)\n",
    "\n",
    "        current_state = event.get(\"dialog_state\")\n",
    "        \n",
    "        message = event.get(\"messages\")\n",
    "        if message:\n",
    "            if isinstance(message, list):\n",
    "                message = message[-1]\n",
    "            if message.type == 'tool':\n",
    "                tool_output += message.content\n",
    "            if message.type == 'ai':\n",
    "                assistant_response += message.content\n",
    "\n",
    "    # Fallback if no response\n",
    "    if not assistant_response:\n",
    "        assistant_response = \"I couldn’t process your request. Please try again.\"\n",
    "\n",
    "    # Save and display assistant's response\n",
    "    st.session_state[\"results\"].append(result)\n",
    "    st.session_state[\"messages\"].append((\"Tool output\", tool_output))\n",
    "    st.session_state[\"messages\"].append((\"Assistant\", assistant_response))\n",
    "\n",
    "# Display the conversation\n",
    "for sender, message in st.session_state[\"messages\"]:\n",
    "    st.markdown(f\"**{sender}:** {message}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
